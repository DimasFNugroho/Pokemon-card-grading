# -*- coding: utf-8 -*-
"""pokemon_warping_steps.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1acQytKjmqYUNwCiBC8j7XVEM4Hpbjtf_
"""

import cv2
import numpy as np
from matplotlib import pyplot as plt
import imutils

def align_image(img1, img2, maxFeatures=50000, keepPercent=2, debug=False):
  # Initiate ORB detector
  orb = cv2.ORB_create()

  # find the keypoints and descriptors with ORB
  kp1, des1 = orb.detectAndCompute(img1,None)
  kp2, des2 = orb.detectAndCompute(img2,None)

  # match the features
  method = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING
  matcher = cv2.DescriptorMatcher_create(method)
  matches = matcher.match(des1, des2, None)
 
  # sort the matches by their distance (the smaller the distance,
  # the "more similar" the features are)
  matches = sorted(matches, key=lambda x:x.distance)
  # keep only the top matches
  keep = int(len(matches) * keepPercent)
  matches = matches[:keep]

  # Visualize
  matchedVis = cv2.drawMatches(img1, kp1, img2, kp2, matches, None)
  matchedVis = imutils.resize(matchedVis, width=1000)

	show_image(debug, matchedVis, show_ratio=0.5)
  if debug == True:
    # Show result
    plt.figure(figsize=(15, 15))
    plt.imshow(matchedVis)
    plt.title('matchedVis')
  
  ptsA = np.zeros((len(matches), 2), dtype="float")
  ptsB = np.zeros((len(matches), 2), dtype="float")
  # loop over the top matches
  for (i, m) in enumerate(matches):
  	# indicate that the two keypoints in the respective images
  	# map to each other
  	ptsA[i] = kp1[m.queryIdx].pt
  	ptsB[i] = kp2[m.trainIdx].pt
  # compute the homography matrix between the two sets of matched
  # points
  (H, mask) = cv2.findHomography(ptsA, ptsB, method=cv2.RANSAC)
  # use the homography matrix to align the images
  (h, w) = img2.shape[:2]
  # align image
  aligned = cv2.warpPerspective(img1, H, (w, h))
  return aligned

# 
# Warp the forground to the image template
#

debug = False
img1 = masked

img2 = cv2.imread("cards/card_full/back_full_1.jpg", cv2.IMREAD_COLOR)
img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)

# calculate the difference of the image dimensions
(h1, w1) = img1.shape[:2]
(h2, w2) = img2.shape[:2]

if h1 > h2:
  scale_percent = h2/h1 # percent of original size
  width = int(img1.shape[1] * scale_percent)
  height = int(img1.shape[0] * scale_percent)
  dim = (width, height)
  # resize image
  img1 = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)
else:
  scale_percent = h1/h2
  width = int(img2.shape[1] * scale_percent)
  height = int(img2.shape[0] * scale_percent)
  dim = (width, height)
  # resize image
  img2 = cv2.resize(img2, dim, interpolation = cv2.INTER_AREA)

# 1. Align image
aligned_image = align_image(img1, img2, debug=True)
if debug == True:
  # Show result
  plt.figure(figsize=(15, 15))
  plt.imshow(aligned_image, cmap='gray')
  plt.title('aligned_image')

# 
# Get contours and use convex hull to
# get the full region of the foreground pixels
#

debug = True
# 1. convert the image to RGB
img = aligned_image
if debug == True:
  # Show RGB image
  plt.figure(figsize=(15, 15))
  plt.imshow(img, cmap='gray')
  plt.title('RGB')

# 2.  Apply Median Blur
median = cv2.medianBlur(img, 5)
if debug == True:
  # Show Median Blur result
  plt.figure(figsize=(15, 15))
  plt.imshow(median)
  plt.title('median')

# 3. Convert the image to gray-scale
gray = cv2.cvtColor(median, cv2.COLOR_BGR2GRAY)
if debug == True:
  # Show grayscale
  plt.figure(figsize=(15, 15))
  plt.imshow(gray, cmap='gray')
  plt.title('grayscale')

# 4. Apply thresholding
(thresh, img_thresh) = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY)
if debug == True:
  # Show thresholding result
  plt.figure(figsize=(15, 15))
  plt.imshow(img_thresh, cmap='gray')
  plt.title('threshold')

# 5. Find the contours
contours, hierarchy = cv2.findContours(img_thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

# 6. Find the convex hull from contours and draw it on the original image.
convex_hull = img_thresh
for i in range(len(contours)):
    hull = cv2.convexHull(contours[i])
    cv2.drawContours(convex_hull, [hull], -1, (255, 0, 0), -1)
if debug == True:
  # Show convex hull result
  plt.figure(figsize=(15, 15))
  plt.imshow(convex_hull, cmap='gray')
  plt.title('convex_hull')

# 
# Calculate the center point of the contour
# and the center point of the card template.
# 
# Estimate the translation of the image to align the
# image to the proper position. This is done to restore the
# cropped pixels of the image
#
# Finally, the translation is applied
#

M = cv2.moments(convex_hull)
cX = int(M["m10"] / M["m00"])
cY = int(M["m01"] / M["m00"])

# compute the center of the contour
convex_hull_rgb = cv2.cvtColor(convex_hull, cv2.COLOR_GRAY2RGB)
# draw the center of the convex hull on the image
cv2.circle(convex_hull_rgb, (cX, cY), 1, (100, 0, 100), -1)

# Show canny edge result
plt.figure(figsize=(15, 15))
plt.imshow(convex_hull_rgb, cmap='gray')
plt.title('convex hull center')

img_center_x = int(convex_hull.shape[1]/2)
img_center_y = int(convex_hull.shape[0]/2)
# Draw the center of the image window
cv2.circle(convex_hull_rgb, (img_center_x, img_center_y), 1, (0, 0, 0), -1)
# Show canny edge result
plt.figure(figsize=(15, 15))
plt.imshow(convex_hull_rgb, cmap='gray')
plt.title('convex hull and ideal card center')

# Prepare the translation matrix
x_shift = 0 
y_shift = 0
if cX < img_center_x:
  x_shift = abs(cX - img_center_x) * 2
if cX >= img_center_x:
  x_shift = abs(cX - img_center_x) * (-2) 

if cY < img_center_y:
  y_shift = abs(cY - img_center_y) * 2
if cY >= img_center_y:
  y_shift = abs(cY - img_center_y) * (-2)

translation_matrix = np.float32([ [1,0,x_shift], [0,1,y_shift] ])   

# translate aligned image
img_translation = cv2.warpAffine(aligned_image, translation_matrix, (img_center_x * 2, img_center_y * 2))

# Show image translation result
plt.figure(figsize=(15, 15))
plt.imshow(img_translation, cmap='gray')
plt.title('translated image')

# 
# The translation contains only the image with the cropped pixels.
# However, the position of the image is assumed to be correct.
# 
# Then, the warping between the previously processed foreground pixels
# and the translated image is applied.
#
# Finally, the cropped pixels is restored, as shown in the following.
#

debug=False
# Normalize image dimension
# calculate the difference of the image dimensions
(h_t, w_t) = img_translation.shape[:2]
(h_o, w_o) = masked.shape[:2]

width = 0
height = 0

if h_t > h_o:
  scale_percent = h_o/h_t # percent of original size
  width = int(img_translation.shape[1] * scale_percent)
  height = int(img_translation.shape[0] * scale_percent)
  dim = (width, height)
  # resize image
  img_translation = cv2.resize(img_translation, dim, interpolation = cv2.INTER_AREA)
if h_o > h_t:
  scale_percent = h_t/h_o
  width = int(masked.shape[1] * scale_percent)
  height = int(masked.shape[0] * scale_percent)
  dim = (width, height)
  # resize image
  masked = cv2.resize(masked, dim, interpolation = cv2.INTER_AREA)

if debug == True:
  # Show result
  plt.figure(figsize=(15, 15))
  plt.imshow(img_translation, cmap='gray')
  plt.title('translated image')

# original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
# 1. Align image
final_aligned_image = align_image(masked, img_translation, debug=True)
if debug == True:
  # Show result
  plt.figure(figsize=(15, 15))
  plt.imshow(final_aligned_image, cmap='gray')
  plt.title('final alignment result')

if debug == True:
  # Show result
  plt.figure(figsize=(15, 15))
  plt.subplot(1, 2, 1), plt.imshow(final_aligned_image)
  plt.subplot(1, 2, 2), plt.imshow(img2)
  plt.show()

def get_center_template(blank_image):
  GREEN_MIN = np.array([36, 25, 25],np.uint8)
  GREEN_MAX = np.array([70, 255,255],np.uint8)

  hsv_img = cv2.cvtColor(blank_image,cv2.COLOR_BGR2HSV)
  frame_threshed = cv2.inRange(hsv_img, GREEN_MIN, GREEN_MAX)

  cnts = cv2.findContours(frame_threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
  cnts = imutils.grab_contours(cnts)

  c=cnts[0] 
	
  M = cv2.moments(c)
  cx = int((M["m10"] / M["m00"]))
  cy = int((M["m01"] / M["m00"]))
  
  return (cx, cy)

def centering_alignment_test(img1, img2, maxFeatures=1000, keepPercent=1, debug=False):
  # Initiate ORB detector
  orb = cv2.ORB_create()

  # find the keypoints and descriptors with SIFT
  kp1, des1 = orb.detectAndCompute(img1,None)
  kp2, des2 = orb.detectAndCompute(img2,None)

  # match the features
  method = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING
  matcher = cv2.DescriptorMatcher_create(method)
  matches = matcher.match(des1, des2, None)
 
  # sort the matches by their distance (the smaller the distance,
  # the "more similar" the features are)
  matches = sorted(matches, key=lambda x:x.distance)
  # keep only the top matches
  keep = int(len(matches) * keepPercent)
  matches = matches[:keep]

  # Visualize
  matchedVis = cv2.drawMatches(img1, kp1, img2, kp2, matches, None)
  matchedVis = imutils.resize(matchedVis, width=1000)

  if debug == True:
    # Show result
    plt.figure(figsize=(15, 15))
    plt.imshow(matchedVis)
    plt.title('matchedVis')
  
  ptsA = np.zeros((len(matches), 2), dtype="float")
  ptsB = np.zeros((len(matches), 2), dtype="float")
  # loop over the top matches
  for (i, m) in enumerate(matches):
  	# indicate that the two keypoints in the respective images
  	# map to each other
  	ptsA[i] = kp1[m.queryIdx].pt
  	ptsB[i] = kp2[m.trainIdx].pt
  # compute the homography matrix between the two sets of matched
  # points
  (H, mask) = cv2.findHomography(ptsA, ptsB, method=cv2.RANSAC)
  
  # use the homography matrix to align the images
  (h, w) = img2.shape[:2]
  # align image
  aligned = cv2.warpPerspective(img1, H, (w, h))
 
  # mask with black image
  blank_image = np.zeros((img1.shape[0], img1.shape[1],3), np.uint8)
  cx = int(img1.shape[1]/2) # last center from template
  cy = int(img1.shape[0]/2) # last center from template

  cv2.circle(blank_image, (cx, cy), 10, (0,255,0), -1)

  blank_image_aligned = cv2.warpPerspective(blank_image, H, (w, h))
  tempalte_cx, template_cy = get_center_template(blank_image_aligned)
  cv2.circle(aligned, (tempalte_cx, template_cy), 1, (0,255,0), -1)

  original_cx, original_cy = int(aligned.shape[1]/2), int(aligned.shape[0]/2)

  a = np.array((tempalte_cx, template_cy))
  b = np.array((original_cx, original_cy))
  distance = dist = np.linalg.norm(a-b)
  
#
# Show the distance between the input image center and the template image center
#
  print('euclidian distance: ', distance)

  return aligned

img2 = cv2.imread("cards/card_full/back_full_1.jpg", cv2.IMREAD_COLOR)
img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)

# Testing the image centering alignment using template
img_template_color = cv2.imread("cards/card_template/back_template_1.png", cv2.IMREAD_COLOR)
img_template = cv2.cvtColor(img_template_color, cv2.COLOR_BGR2RGB)
# 1. Align image
img4 = centering_alignment_test(img_template, final_aligned_image, keepPercent=1, debug=True)
(h4, w4) = img4.shape[:2]

cv2.circle(img4,(int(w4/2),int(h4/2)), 1, (255,0,0), -1)

# img4[int(h4/2), int(w4/2)]=[255,0,0]
# Show result
plt.figure(figsize=(25, 25))
plt.imshow(img4)
plt.title('img4')
